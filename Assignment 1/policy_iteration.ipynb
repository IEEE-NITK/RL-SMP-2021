{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "policy_iteration.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N06bfHFruw-U"
      },
      "source": [
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5-6Prr4uw-W"
      },
      "source": [
        "def initialization():\n",
        "    '''\n",
        "    Returns:\n",
        "            V - a 2d array initialized as 0\n",
        "            R - array containing rewards for each state\n",
        "            P - array denoting equiprobable random policy\n",
        "            states - array containing tuples of states in the gridworld\n",
        "            terminal_states - array containing terminal states of the gridworld\n",
        "    '''\n",
        "    num_rows = 4\n",
        "    num_cols = 4\n",
        "    states = []\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            states.append((i,j))\n",
        "    terminal_states = [(0,0),(3,3)]\n",
        "    V = np.zeros([4,4])\n",
        "    R = {}\n",
        "    P = {}\n",
        "    for state in states:\n",
        "        if state in terminal_states:\n",
        "            R[state] = 0\n",
        "            P[state] = []\n",
        "        else:\n",
        "            R[state] = -1\n",
        "            P[state] = ['L','R','D','U']\n",
        "    return V,R,P,states,terminal_states"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXjzp5mvw5DM"
      },
      "source": [
        "def get_state(action, state):\n",
        "  if action == 'L':\n",
        "    new_state = (state[0], state[1]-1)\n",
        "  if action == 'R':\n",
        "    new_state = (state[0], state[1]+1)\n",
        "  if action == 'U':\n",
        "    new_state = (state[0]-1, state[1])\n",
        "  if action == 'D':\n",
        "    new_state = (state[0]+1, state[1])\n",
        "  if new_state not in states:\n",
        "    new_state = state\n",
        "  return new_state"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAXkmXHEwtsm"
      },
      "source": [
        "def RewardFunction(state, action, R, states):\n",
        "    reward = R[state]\n",
        "    finalPosition = get_state(action,state)\n",
        "    return finalPosition, reward"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTisGn2wuw-X"
      },
      "source": [
        "def policy_evaluation(V,R,P,states,terminal_states):\n",
        "    '''\n",
        "    Arguments:\n",
        "            V is a 2d array initialized as 0\n",
        "            R is the array containing rewards for each state\n",
        "            P is the policy taken by the agent\n",
        "            states - array containing tuples of states in the gridworld\n",
        "            terminal_states - array containing terminal states of the gridworld\n",
        "    Returns:\n",
        "            V - The value function calculated for the policy P\n",
        "    '''\n",
        "    num_iterations = 0\n",
        "    V = {}\n",
        "    for state in states:\n",
        "        V[state] = 0\n",
        "    gamma = 1\n",
        "    while num_iterations < 2000:\n",
        "        deltas = []\n",
        "        copyV = copy.deepcopy(V)\n",
        "        deltaState = []\n",
        "        for state in states:\n",
        "          new_reward = 0\n",
        "          actions = P[state]\n",
        "          for action in actions:\n",
        "            final_state, reward = RewardFunction(state, action, R, states)\n",
        "            new_reward += (1/len(actions))*(reward+(gamma*V[final_state]))          \n",
        "          deltaState.append(np.abs(copyV[state]-new_reward))\n",
        "          copyV[state] = new_reward\n",
        "        deltas.append(deltaState)\n",
        "        V = copyV\n",
        "        num_iterations += 1\n",
        "        info = False\n",
        "        if info:\n",
        "          if num_iterations in [0,1,2,9, 99, 1999]:\n",
        "            print(\"Iteration {}\".format(num_iterations+1))\n",
        "            print(V)\n",
        "            print(\"\")\n",
        "    return V"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJr84PUH2xXp"
      },
      "source": [
        "V,R,P1,states,terminal_states = initialization()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mOI0Pg62rQZ",
        "outputId": "386fd157-33e7-4c5e-d625-17a1889e0fed"
      },
      "source": [
        "'''Testing Policy Evaluation\n",
        "\n",
        "\n",
        "P_t = [ [[], ['L'], ['L'], ['L', 'D']], [['U'], ['L', 'U'], ['L', 'R', 'U', 'D'], ['D']], [['U'], ['L', 'R', 'U', 'D'], ['R', 'D'], ['D']], [['R', 'U'], ['R'], ['R'], []]]\n",
        "P_test = {}\n",
        "for row in range(4):\n",
        "  for col in range(4):\n",
        "    s = (row,col)\n",
        "    P_test[s] =  P_t[row][col]\n",
        "print(P_test)\n",
        "v_test = policy_evaluation(V, R, P_test, states, terminal_states)\n",
        "V2 = np.zeros((4,4))\n",
        "for state in states:\n",
        "    V2[state[0]][state[1]] = v_test[state]\n",
        "print(V2)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(0, 0): [], (0, 1): ['L'], (0, 2): ['L'], (0, 3): ['L', 'D'], (1, 0): ['U'], (1, 1): ['L', 'U'], (1, 2): ['L', 'R', 'U', 'D'], (1, 3): ['D'], (2, 0): ['U'], (2, 1): ['L', 'R', 'U', 'D'], (2, 2): ['R', 'D'], (2, 3): ['D'], (3, 0): ['R', 'U'], (3, 1): ['R'], (3, 2): ['R'], (3, 3): []}\n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRtdrayvCzbu"
      },
      "source": [
        "def get_array(V, R, P, state):\n",
        "  actions = P[state]\n",
        "  V_array = []\n",
        "  for action in actions:\n",
        "    new_state = get_state(action, state)\n",
        "    V_state = R[state] + V[new_state]\n",
        "    V_array.append(round(V_state, 2))\n",
        "  return V_array\n",
        "\n",
        "def get_indices(arr, max_):\n",
        "  indices = []\n",
        "  for x in range(len(arr)):\n",
        "    if arr[x] == max_:\n",
        "      indices.append(x)\n",
        "  return indices"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWTtdPOzuw-Y"
      },
      "source": [
        "def policy_improvement(V,R,P,P1,states,terminal_states):\n",
        "    '''\n",
        "    Arguments: \n",
        "            V is the value function\n",
        "            R is the array containing rewards for each state\n",
        "            P is the equiprobable random policy\n",
        "            P1 is the previous optimal policy\n",
        "            states - array containing tuples of states in the gridworld\n",
        "            terminal_states - array containing terminal states of the gridworld\n",
        "    Returns:\n",
        "            P - Optimal policy after performing policy improvement\n",
        "            policy_stable - bool variable denoting if P = P1\n",
        "    '''\n",
        "    policy_stable = False\n",
        "    # Iterate over all states to find the optimal policy\n",
        "    for state in states:\n",
        "      V_array = get_array(V, R, P1, state)\n",
        "      if len(V_array):\n",
        "        max_ = max(V_array)\n",
        "        indices = get_indices(V_array, max_)\n",
        "        a = P1[state]\n",
        "        P[state] = [a[indice] for indice in indices] \n",
        "    if P == P1:\n",
        "      policy_stable = True\n",
        "    return P,policy_stable"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4MOdZW1uw-Y",
        "outputId": "af9d213f-88fd-46f3-83f9-792b20737b3d"
      },
      "source": [
        "V = policy_evaluation(V,R,P1,states,terminal_states)\n",
        "\n",
        "# Initial value function\n",
        "V1 = np.zeros((4,4))\n",
        "for state in states:\n",
        "    V1[state[0]][state[1]] = V[state]\n",
        "print(\"Initial value function is: \")\n",
        "print(np.round(V1))\n",
        "\n",
        "# Perform policy iteration until the policy doesn't change for any state in an iteration\n",
        "\n",
        "while True:\n",
        "    # Equiprobable random policy\n",
        "    P = {}\n",
        "    for state in states:\n",
        "        if not (state in terminal_states):\n",
        "            P[state] = ['L','R','U','D']\n",
        "        else:\n",
        "            P[state] = []\n",
        "\n",
        "    P1,policy_stable = policy_improvement(V,R,P,P1,states,terminal_states)\n",
        "    # If policy stable is true, the policy hasn't changed for any state in an iteration\n",
        "    if policy_stable:\n",
        "        break\n",
        "    V = policy_evaluation(V,R,P1,states,terminal_states)\n",
        "\n",
        "# Print optimal value function\n",
        "print(\"\\nOptimal value function is: \")\n",
        "V1 = np.zeros((4,4))\n",
        "for state in states:\n",
        "    V1[state[0]][state[1]] = V[state]\n",
        "print(np.round(V1))\n",
        "\n",
        "\n",
        "# Print optimal policy\n",
        "# Each cell denotes the optimal action that needs to be taken from that state\n",
        "P = [[[],[],[],[]],\n",
        "     [[],[],[],[]],\n",
        "     [[],[],[],[]],\n",
        "     [[],[],[],[]]]\n",
        "for v,a in P1.items():\n",
        "    P[v[0]][v[1]] = a\n",
        "print(\"\\nThe optimal policy is:\")\n",
        "for row in P:\n",
        "    print(row)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial value function is: \n",
            "[[  0. -14. -20. -22.]\n",
            " [-14. -18. -20. -20.]\n",
            " [-20. -20. -18. -14.]\n",
            " [-22. -20. -14.   0.]]\n",
            "\n",
            "Optimal value function is: \n",
            "[[ 0. -1. -2. -3.]\n",
            " [-1. -2. -3. -2.]\n",
            " [-2. -3. -2. -1.]\n",
            " [-3. -2. -1.  0.]]\n",
            "\n",
            "The optimal policy is:\n",
            "[[], ['L'], ['L'], ['L', 'D']]\n",
            "[['U'], ['L', 'U'], ['L', 'D'], ['D']]\n",
            "[['U'], ['R', 'U'], ['R', 'D'], ['D']]\n",
            "[['R', 'U'], ['R'], ['R'], []]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}