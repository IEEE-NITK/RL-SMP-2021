{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA and Q-learning in windy gridworld\n",
    "***\n",
    "### The gridworld:\n",
    "<img src=\"windy_gridworld.png\" alt=\"The windy gridword\" title=\"Windy gridworld\" width=\"400\"/>\n",
    "\n",
    "#### Notation followed: \n",
    "Note that here that the state (cell in the grid) is defined by a single numeric value.<br>\n",
    "The numbering starts from 0 (top left cell) and goes till 69 (bottom left). Cell 30 is the start state and 37 is the end state. <br><br>\n",
    "\n",
    "The reward for each step (action chosen) is -1.<br><br>\n",
    "\n",
    "The 'wind' is represented by the numbers below the grid. For each move from a state (S), depending on the action, the agent moves and then the environment moves it as many cells up as indicated by the 'wind' numberof the column of S.<br><br>\n",
    "\n",
    "For example: if the agent were in the state 47 (right below the end state) and chose to move right, then it would go one cell to the right and be blown 2 cells upward, finally ending in state 28 (upper right of the end state). Note that the reward for this step would still be -1.\n",
    "\n",
    "#### Choice of actions:\n",
    " - 0: Up\n",
    " - 1: Down\n",
    " - 2: Right\n",
    " - 3: Left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public variables about the env\n",
    "START_STATE = 30\n",
    "END_STATE = 37\n",
    "rows = 7\n",
    "columns = 10\n",
    "num_states = rows*columns\n",
    "num_actions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function windy returns the value of 'wind' given a state.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windy(state):\n",
    "    # <write code below>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function env_step takes in current state and action as input.<br>\n",
    "Samples one step of the environment. <br>\n",
    "Returns reward and the next state the agent ends up at.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(state, action):\n",
    "    wind = windy(state) \n",
    "    # transition to next state depending on action and wind in current state <write code below>\n",
    "    \n",
    "    return [next_state, reward]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha, epsilon, gamma, episodes) :\n",
    "    # initialize action values (Q) to a np matrix  of 0s <write code below>\n",
    "    Q = \n",
    "    for episode in range(episodes):\n",
    "        curr_state = START_STATE\n",
    "        \n",
    "        # choose curr_action based on epsilon greedy policy over Q <write code below>\n",
    "        \n",
    "        while curr_state!=END_STATE:\n",
    "            # sample a step of the environment\n",
    "            next_state, reward = env_step(curr_state, curr_action)\n",
    "            # choose next_action based on epsilon greedy poicy over Q <write code below>\n",
    "            \n",
    "            # update Q <write code below>\n",
    "            Q[curr_state, curr_action] += \n",
    "            \n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(alpha, epsilon, gamma, episodes):\n",
    "    # initialize action values (Q) to a np matrix  of 0s <write code below>\n",
    "    Q = \n",
    "    for episode in range(episodes): \n",
    "        curr_state = START_STATE\n",
    "        \n",
    "        while curr_state != END_STATE:\n",
    "            # choose curr_action based on epsilon greedy policy over Q <write code below>\n",
    "            \n",
    "            # sample a step of the environment\n",
    "            next_state, reward = env_step(curr_state, curr_action)\n",
    "            \n",
    "            # update Q <write code below>\n",
    "            Q[curr_state, curr_action] += \n",
    "            \n",
    "            curr_state = next_state\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public variables setting the parameters \n",
    "alpha = 0.3\n",
    "epsilon = 0.1\n",
    "gamma = 1 # undiscounted task\n",
    "episodes = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_func_sarsa = sarsa(alpha, epsilon, gamma, episodes)\n",
    "value_func_q = Q_learning(alpha, epsilon, gamma, episodes)\n",
    "# print (\"value function after sarsa:  \\n\\n\", value_func )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function greedy_policy taken in an action value function as input.<br>\n",
    "For each state, the value of pi[state] is the action which has the max action value in that state.<br>\n",
    "The function returns the greedy policy over the action value function, pi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Q):\n",
    "    # initialize pi\n",
    "    pi = np.zeros((rows, columns))\n",
    "    # calculate pi <write code below>\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to print the policy in a readable manner, given the greedy policy\n",
    "def print_policy(policy):\n",
    "    number_to_action = ['U', 'D', 'R', 'L']\n",
    "    for i in range(rows):\n",
    "        for j in range (columns):\n",
    "            print(number_to_action[int(policy[i][j])], end='  ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy learnt using SARSA:\n",
      "\n",
      "D  L  D  R  R  R  R  R  R  D  \n",
      "R  R  R  R  R  R  R  R  U  D  \n",
      "R  R  R  R  R  R  D  R  R  D  \n",
      "R  R  R  R  R  R  R  U  R  D  \n",
      "R  D  R  R  U  R  U  D  L  L  \n",
      "R  R  D  R  R  U  U  D  L  U  \n",
      "R  R  R  R  U  U  U  U  U  L  \n"
     ]
    }
   ],
   "source": [
    "sarsa_policy = greedy_policy(value_func_sarsa)\n",
    "print(\"policy learnt using SARSA:\\n\")\n",
    "print_policy(sarsa_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy learnt using Q-learning:\n",
      "\n",
      "D  R  R  R  R  R  R  R  R  D  \n",
      "R  R  R  R  R  R  R  R  R  D  \n",
      "R  R  R  R  R  R  R  L  R  D  \n",
      "R  R  R  R  R  R  R  U  R  D  \n",
      "R  R  R  R  R  R  U  D  L  L  \n",
      "U  R  R  R  R  U  U  D  L  U  \n",
      "D  R  D  R  U  U  U  U  D  L  \n"
     ]
    }
   ],
   "source": [
    "q_policy = greedy_policy(value_func_q)\n",
    "print(\"policy learnt using Q-learning:\\n\")\n",
    "print_policy(q_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual of the policy learnt:\n",
    "By tracing the motion of the agent from the start_state to the end_state, verify that both SARSA and Q-leaning result in a policy , following which you get the below path.<br>\n",
    "<img src=\"windy_path.png\" alt=\"The windy gridword\" title=\"Windy gridworld\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sample an episode following a psrticular policy\n",
    "# returns the 'return' obtained in that episode\n",
    "def run(policy):\n",
    "    curr_state = START_STATE\n",
    "    R = 0\n",
    "    while curr_state!=END_STATE:\n",
    "        curr_state,reward= env_step(curr_state, policy[int(curr_state/10)][curr_state%10])\n",
    "        R+=reward\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"return from following optimal SARSA policy: \", run(sarsa_policy))\n",
    "print (\"return from following optimal Q-learning policy: \", run(q_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the return from both should be = -15.<br><br>\n",
    "If you've got that then congrats you have successfully implemented SARSA and Q-learning algorithms!<br><br>\n",
    "You can go back to the cell where parameters like alpha and number of episodes are defined. Change their values and run the cells to see how they affect the result and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
